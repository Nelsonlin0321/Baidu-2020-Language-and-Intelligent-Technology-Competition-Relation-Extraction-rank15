{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用于ensemble\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import pickle as pk\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display,HTML\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import math\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from transformers import *\n",
    "import json\n",
    "device=torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.set_printoptions(suppress=True, threshold=np.nan)\n",
    "pd.set_option('float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     10,
     41,
     51,
     74
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "do_lower_case=True\n",
    "max_len=256\n",
    "device=torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "# bert_dir=\"./bert-pytorch-chinese/\"\n",
    "# vocab=\"bert-base-chinese-vocab.txt\"\n",
    "# config_file=\"bert_config.json\"\n",
    "bert_dir=\"./roberta-zh-wwm-pytorch/\"\n",
    "vocab=\"vocab.txt\"\n",
    "config_file=\"bert_config.json\"\n",
    "tokenizer=BertTokenizer.from_pretrained(os.path.join(bert_dir,vocab),do_lower_case=do_lower_case)\n",
    "def index_token(text_raw):\n",
    "    sub_text = []\n",
    "    buff = \"\"\n",
    "    for char in text_raw:\n",
    "        if is_chinese_char(ord(char)) or is_punctuation(char):\n",
    "            if buff != \"\":\n",
    "                sub_text.append(buff)\n",
    "                buff = \"\"\n",
    "            sub_text.append(char)\n",
    "        else:\n",
    "            buff += char\n",
    "    if buff != \"\":\n",
    "        sub_text.append(buff)\n",
    "    tok_to_orig_start_index = []\n",
    "    tok_to_orig_end_index = []\n",
    "    orig_to_tok_index = []\n",
    "    tokens = []\n",
    "    text_tmp = ''\n",
    "    for (i, token) in enumerate(sub_text):\n",
    "        orig_to_tok_index.append(len(tokens))\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "        text_tmp += token\n",
    "        for sub_token in sub_tokens:\n",
    "            tok_to_orig_start_index.append(len(text_tmp) - len(token))\n",
    "            tok_to_orig_end_index.append(len(text_tmp) - 1)\n",
    "            tokens.append(sub_token)\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    return tok_to_orig_start_index,tok_to_orig_end_index,orig_to_tok_index\n",
    "import unicodedata\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "def is_chinese_char(cp):\n",
    "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "    #\n",
    "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "    # space-separated words, so they are not treated specially and handled\n",
    "    # like the all of the other languages.\n",
    "    if (\n",
    "        (cp >= 0x4E00 and cp <= 0x9FFF)\n",
    "        or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
    "        or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
    "        or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
    "        or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
    "        or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
    "        or (cp >= 0xF900 and cp <= 0xFAFF)\n",
    "        or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
    "    ):  #\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "def is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     6,
     11,
     16,
     22,
     29,
     36,
     46,
     67,
     76
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dict...\n",
      "loading...\n"
     ]
    }
   ],
   "source": [
    "text_data=[]\n",
    "with open(\"./dataset/train_data/new_train_data.json\",\"r\") as r:\n",
    "    raw_data=r.readlines()\n",
    "    for d in raw_data:\n",
    "        text_data.append(json.loads(d))\n",
    "test_text_data=[]\n",
    "with open(\"./dataset/test1_data/new_test1_data.json\",\"r\") as r:\n",
    "    raw_data=r.readlines()\n",
    "    for d in raw_data:\n",
    "        test_text_data.append(json.loads(d))\n",
    "test2_text_data=[]\n",
    "with open(\"./dataset/test2_data/new_test2_data.json\",\"r\") as r:\n",
    "    raw_data=r.readlines()\n",
    "    for d in raw_data:\n",
    "        test2_text_data.append(json.loads(d))\n",
    "schema=[]\n",
    "with open(\"./dataset/schema.json\",\"r\") as r:\n",
    "    raw_schema=r.readlines()\n",
    "    for d in raw_schema:\n",
    "        schema.append(json.loads(d))\n",
    "rels=set()\n",
    "special_rels=set()\n",
    "for e in schema:\n",
    "        if len(e['object_type'].keys())==1:\n",
    "            rels.add(e[\"predicate\"])\n",
    "        else:\n",
    "            special_rels.add(e[\"predicate\"])\n",
    "            for key in e['object_type'].keys():\n",
    "                rels.add(e['predicate']+\"_\"+key)\n",
    "if not os.path.exists(\"./dataset/dict.pk\"):\n",
    "    special_rels=list(special_rels)\n",
    "    id2rels=list(rels)\n",
    "    rels2id=dict([(rel,idx) for idx,rel in enumerate(id2rels)])\n",
    "    id2labels=[\"O\",\"B-OBJ\",\"I-OBJ\",\"B-SUB\",\"I-SUB\",\"[category]\",\"[SEP]\",\"[CLS]\",\"[PAD]\"]\n",
    "    label2ids=dict([ (label,idx) for idx,label in enumerate(id2labels)])\n",
    "    pk.dump([special_rels,id2rels,rels2id,id2labels,label2ids],open(\"./dataset/dict.pk\",\"wb\"))\n",
    "else:\n",
    "    print(\"loading dict...\")\n",
    "    special_rels,id2rels,rels2id,id2labels,label2ids=pk.load(open(\"./dataset/dict.pk\",\"rb\"))\n",
    "id2kglabels=['O','I']+['B-'+e+\"-SUB\" for e in id2rels]+['B-'+e+\"-OB\" for e in id2rels]\n",
    "kglabels2id=dict([ (label,idx) for idx,label in enumerate(id2kglabels)])\n",
    "new_id2labels=[\"O\",\"B-OBJ\",\"E-OBJ\",\"B-SUB\",\"E-SUB\",\"[category]\",\"[SEP]\",\"[CLS]\",\"[PAD]\"]\n",
    "new_label2ids=dict([ (label,idx) for idx,label in enumerate(new_id2labels)])\n",
    "new_id2kglabels=['O']+['B-'+e+\"-SUB\" for e in id2rels]+['B-'+e+\"-OB\" for e in id2rels]+['E-'+e+\"-SUB\" for e in id2rels]+['E-'+e+\"-OB\" for e in id2rels]\n",
    "new_kglabels2id=dict([ (label,idx) for idx,label in enumerate(new_id2kglabels)])\n",
    "id2reltype=[[] for i in range(len(id2rels))]\n",
    "for e in schema:\n",
    "    if len(e['object_type'].keys())==1:\n",
    "        rel=e[\"predicate\"]\n",
    "        ids=rels2id[rel]\n",
    "        id2reltype[ids].append(e)\n",
    "    else:\n",
    "        for key in e['object_type'].keys():\n",
    "            rel=e['predicate']+\"_\"+key\n",
    "            ids=rels2id[rel]\n",
    "            temp_e=copy.deepcopy(e)\n",
    "            poped_keys=[]\n",
    "            for k in temp_e['object_type'].keys():\n",
    "                if k!=key:\n",
    "                    poped_keys.append(k)\n",
    "            for k in poped_keys:\n",
    "                 temp_e['object_type'].pop(k)\n",
    "            id2reltype[ids].append(temp_e)\n",
    "id2schema=[e[0] for e in id2reltype]\n",
    "id2rel_text=[[] for i in range(len(id2rels))]\n",
    "id2rel_rawtext=[[] for i in range(len(id2rels))]\n",
    "id2rel_token2text=[[] for i in range(len(id2rels))]\n",
    "for rel in range(len(id2rels)):\n",
    "    if id2rels[rel].split(\"_\")[0] not in special_rels:\n",
    "        cls_text=id2schema[rel]['subject_type']+\",\"+id2schema[rel]['predicate']+\",\"+id2schema[rel]['object_type']['@value']\n",
    "    else:\n",
    "        cls_text=id2schema[rel]['subject_type']+\",\"+id2schema[rel]['predicate']+\",\"+id2schema[rel]['object_type'][id2rels[rel].split(\"_\")[1]]\n",
    "    id2rel_text[rel]=tokenizer.tokenize(cls_text)\n",
    "    id2rel_rawtext[rel]=cls_text\n",
    "    id2rel_token2text[rel]=index_token(cls_text)[0]\n",
    "    assert len(id2rel_token2text[rel])==len(id2rel_text[rel])\n",
    "if not os.path.exists(\"./middle_data/rel_data_postag.pk\"):\n",
    "    jieba.enable_paddle() \n",
    "    jieba.enable_parallel(8)\n",
    "    rel_cut_words=[]\n",
    "    rel_cut_tags=[]\n",
    "    for idx in tqdm(range(len(id2rel_rawtext))):\n",
    "        words = pseg.lcut(id2rel_rawtext[idx],use_paddle=True) #jieba默认模式\n",
    "        new_words=[w for w,t in words]\n",
    "        new_tags=[t for w,t in words]\n",
    "        rel_cut_words.append([idx,new_words])\n",
    "        rel_cut_tags.append([idx,new_tags])\n",
    "    rel_cut_words=[e[1] for e in sorted(rel_cut_words,key=lambda x:x[0])]\n",
    "    rel_cut_tags=[e[1] for e in sorted(rel_cut_tags,key=lambda x:x[0])]\n",
    "    rel_data_postag=[]\n",
    "    for idx in tqdm(range(len(id2rel_rawtext))):\n",
    "        assert len(id2rel_rawtext[idx].strip())==len(\"\".join(rel_cut_words[idx]))\n",
    "        indexs=[]\n",
    "        cur_length=0\n",
    "        for e in rel_cut_words[idx]:\n",
    "            indexs.append([cur_length,cur_length+len(e)-1])\n",
    "            cur_length+=len(e)\n",
    "        pos_label=np.zeros(len(id2rel_rawtext[idx])).astype(np.int8)\n",
    "        for i,(b,e) in enumerate(indexs):\n",
    "            assert (id2rel_rawtext[idx][b]==rel_cut_words[idx][i][0] or _is_whitespace(id2rel_rawtext[idx].strip()[b])\\\n",
    "                   or _is_whitespace(rel_cut_words[idx][i][0])) \\\n",
    "                    and (id2rel_rawtext[idx].strip()[e]==rel_cut_words[idx][i][-1] \\\n",
    "                         or _is_whitespace(id2rel_rawtext[idx].strip()[e])  or _is_whitespace(rel_cut_words[idx][i][-1]))\n",
    "            pos_label[b+1:e+1]=pos2id_BIO['I-'+rel_cut_tags[idx][i]]\n",
    "            pos_label[b]=pos2id_BIO['B-'+rel_cut_tags[idx][i]]\n",
    "        rel_data_postag.append(pos_label)\n",
    "    pk.dump(rel_data_postag,open(\"./middle_data/rel_data_postag.pk\",\"wb\"))\n",
    "else:\n",
    "    print(\"loading...\")\n",
    "    rel_data_postag=pk.load(open(\"./middle_data/rel_data_postag.pk\",\"rb\"))\n",
    "special_major_idx=[2,4,22,32,54]\n",
    "special_affilate_idx=[[] for i in range(len(id2rels))]\n",
    "special_affilate_idx[2]=[5]\n",
    "special_affilate_idx[4]=[0]\n",
    "special_affilate_idx[22]=[51]\n",
    "special_affilate_idx[32]=[8]\n",
    "special_affilate_idx[54]=[6,11,14]\n",
    "special_pass_idx=[0,5,6,8,11,14,51]\n",
    "padding_O=[0]*len(new_id2labels)\n",
    "padding_O[0]=1\n",
    "padding_category=[0]*len(new_id2labels)\n",
    "padding_category[5]=1\n",
    "padding_SEP=[0]*len(new_id2labels)\n",
    "padding_SEP[6]=1\n",
    "padding_CLS=[0]*len(new_id2labels)\n",
    "padding_CLS[7]=1\n",
    "padding_PAD=[0]*len(new_id2labels)\n",
    "padding_PAD[8]=1 #for pointer ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "with open(\"./cross_results/train_info.json\",\"r\") as r:\n",
    "    raw_data=r.readlines()\n",
    "    for d in raw_data:\n",
    "        train_data.append(json.loads(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     4,
     10
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c047fed57f374fe7a13d4643ab3f7b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_X=[]\n",
    "train_Y=[]\n",
    "full_rel_nums=np.zeros((len(train_data)))\n",
    "rel_index=[]\n",
    "for i,e in tqdm(enumerate(train_data)):\n",
    "    cls_logits=np.array(e['cls_logits'])\n",
    "    triple_num=len(e['spo_list'])\n",
    "    rels=set()\n",
    "    entities_num=0\n",
    "    full_rel_nums[i]=len(text_data[i]['spo_list'])\n",
    "    for spo in e['spo_list']:\n",
    "        rels.add(spo['predicate'])\n",
    "        entities_num+=(1+len(spo['object']))\n",
    "    rel_num=len(rels)\n",
    "    for idx in range(len(e['spo_list'])):\n",
    "        spo=e['spo_list'][idx]\n",
    "        spo_logits=e['spo_logits'][idx]\n",
    "        sub_ob_logits=np.concatenate([np.array(spo_logits['subject'])[0],np.array(spo_logits['subject'])[-1],\\\n",
    "                              np.array(spo_logits['object'])[0],np.array(spo_logits['object'])[-1]])\n",
    "        predicate=spo['predicate'] if spo['predicate'] not in special_rels else spo['predicate']+'_@value'\n",
    "        sub=spo['subject']\n",
    "        sub_len=len(sub)\n",
    "        ob=spo['object']['@value']\n",
    "        ob_len=len(ob)\n",
    "        show_time=0\n",
    "        for plan_spo in e['plan_spo_list']:\n",
    "            if plan_spo['predicate']==predicate and plan_spo['subject']==sub and plan_spo['object']==ob:\n",
    "                show_time+=1\n",
    "                break\n",
    "        feature=np.concatenate([sub_ob_logits,np.array([show_time,rel_num,entities_num,triple_num]),np.array([sub_len,ob_len])])\n",
    "        label=0\n",
    "        if spo in text_data[i]['spo_list']:\n",
    "            label=1\n",
    "        train_X.append(feature)\n",
    "        train_Y.append(label)\n",
    "        rel_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=np.array(train_X)\n",
    "train_Y=np.array(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44355.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_rel_nums[rel_index[350000]:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6878920283522673, 0.7986247322737008, 0.7391340636411059)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=35423/51495\n",
    "r=35423/44355\n",
    "f1=(2*p*r)/(p+r)\n",
    "p,r,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35395"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((val_pre[350000:]>0.05)==1) & (train_Y[350000:]==1)).sum() #TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47306, 35423)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((val_pre[350000:]>0.05)==1).sum(),(train_Y[350000:]==1).sum() #P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttrain's auc: 0.921881\tval's auc: 0.922351\n",
      "[200]\ttrain's auc: 0.926029\tval's auc: 0.926458\n",
      "[300]\ttrain's auc: 0.928586\tval's auc: 0.929064\n",
      "[400]\ttrain's auc: 0.930687\tval's auc: 0.931196\n",
      "[500]\ttrain's auc: 0.932144\tval's auc: 0.932665\n",
      "[600]\ttrain's auc: 0.933352\tval's auc: 0.933869\n",
      "[700]\ttrain's auc: 0.934148\tval's auc: 0.934675\n",
      "[800]\ttrain's auc: 0.934822\tval's auc: 0.935353\n",
      "[900]\ttrain's auc: 0.935484\tval's auc: 0.935981\n",
      "[1000]\ttrain's auc: 0.936137\tval's auc: 0.936595\n",
      "[1100]\ttrain's auc: 0.936767\tval's auc: 0.937212\n",
      "[1200]\ttrain's auc: 0.93735\tval's auc: 0.937798\n",
      "[1300]\ttrain's auc: 0.937859\tval's auc: 0.938297\n",
      "[1400]\ttrain's auc: 0.938372\tval's auc: 0.938798\n",
      "[1500]\ttrain's auc: 0.938831\tval's auc: 0.939262\n",
      "[1600]\ttrain's auc: 0.939305\tval's auc: 0.939727\n",
      "[1700]\ttrain's auc: 0.939794\tval's auc: 0.940211\n",
      "[1800]\ttrain's auc: 0.94032\tval's auc: 0.940724\n",
      "[1900]\ttrain's auc: 0.940779\tval's auc: 0.941195\n",
      "[2000]\ttrain's auc: 0.941194\tval's auc: 0.941603\n",
      "[2100]\ttrain's auc: 0.941672\tval's auc: 0.94206\n",
      "[2200]\ttrain's auc: 0.942135\tval's auc: 0.942505\n",
      "[2300]\ttrain's auc: 0.942561\tval's auc: 0.942938\n",
      "[2400]\ttrain's auc: 0.943019\tval's auc: 0.943386\n",
      "[2500]\ttrain's auc: 0.943413\tval's auc: 0.94376\n",
      "[2600]\ttrain's auc: 0.943812\tval's auc: 0.944158\n",
      "[2700]\ttrain's auc: 0.944161\tval's auc: 0.944504\n",
      "[2800]\ttrain's auc: 0.944496\tval's auc: 0.944837\n",
      "[2900]\ttrain's auc: 0.944827\tval's auc: 0.945163\n",
      "[3000]\ttrain's auc: 0.945185\tval's auc: 0.945517\n",
      "[3100]\ttrain's auc: 0.945516\tval's auc: 0.945839\n",
      "[3200]\ttrain's auc: 0.945842\tval's auc: 0.946159\n",
      "[3300]\ttrain's auc: 0.946194\tval's auc: 0.9465\n",
      "[3400]\ttrain's auc: 0.946561\tval's auc: 0.946869\n",
      "[3500]\ttrain's auc: 0.946901\tval's auc: 0.947203\n",
      "[3600]\ttrain's auc: 0.947217\tval's auc: 0.947515\n",
      "[3700]\ttrain's auc: 0.947567\tval's auc: 0.947864\n",
      "[3800]\ttrain's auc: 0.947914\tval's auc: 0.948197\n",
      "[3900]\ttrain's auc: 0.948248\tval's auc: 0.948528\n",
      "[4000]\ttrain's auc: 0.948528\tval's auc: 0.948804\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4000]\ttrain's auc: 0.948528\tval's auc: 0.948804\n",
      "303.04620599746704\n"
     ]
    }
   ],
   "source": [
    "data_train = lgb.Dataset(train_X[:],train_Y[:],free_raw_data=False)\n",
    "data_val=lgb.Dataset(train_X[350000:],train_Y[350000:],free_raw_data=False)\n",
    "import time\n",
    "#不带encode 的参数\n",
    "param = {\n",
    "    'objective': 'binary', \n",
    "    'boost': 'gbdt',\n",
    "    'metric':'auc',\n",
    "    #初始参数\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 128,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 0,\n",
    "    'feature_fraction': 1,\n",
    "    'num_threads': 8,\n",
    "    'lambda_l1':1e-05,\n",
    "    'lambda_l2':1e-05,\n",
    "    'min_split_gain':0,\n",
    "    'tree_learner': 'serial',\n",
    "    'verbosity': -1,\n",
    "    'device': 'cpu',\n",
    "    'learning_rate': 0.01,\n",
    "}\n",
    "start = time.time()\n",
    "lgb_clf = lgb.train(param, data_train, 4000, valid_sets = [data_train,data_val],\\\n",
    "                    valid_names=[\"train\",\"val\"],verbose_eval=100,early_stopping_rounds=500)# early_stopping_rounds=200\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf=lgb.Booster(model_file=\"./model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8100698638837346\n",
      "auc: 0.9485282139174224\n"
     ]
    }
   ],
   "source": [
    "val_pre=lgb_clf.predict(train_X[:])\n",
    "print(\"acc:\",metrics.accuracy_score(train_Y[:],val_pre>0.1))\n",
    "print(\"auc:\",metrics.roc_auc_score(train_Y[:],val_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7fc45438b5c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf.save_model('model.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fjw",
   "language": "python",
   "name": "fjw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
